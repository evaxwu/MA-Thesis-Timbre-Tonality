---
title: "Exploratory Analysis"
author: "Eva Wu"
output: github_document
date: '2022-05-18'
---

```{r setup, include = FALSE}
library(tidyverse)
library(colorspace)
library(ISLR2)
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

## Recap

### Hypothesis 

“Happy” instruments would make people more prone to identify the chord as major, 
while “sad” instruments might make people more prone to identify the chord as minor.

### Exploratory research questions 

1) Association between timbre and tonality judgment 

2) Association between timbre and explicit ratings of instrument valence

3) Association between tonality judgment and explicit ratings of instrument valence

4) Association between musical background and tonality judgment and/or explicit ratings of instrument valence

### Design

* IV1 (w/in-subject): instrument (happy [xylophone, trumpet] vs. neutral [piano] vs. sad [oboe, violin]) 

* IV2 (w/in-subject): tuning of middle note (5 levels, ranging from absolute minor to absolute major)

* IV3 (b/w-subject): key (B vs. C) (to find out absolute-pitch-related effects)

* DV: the likelihood that one categorizes a chord as major/minor 

### Procedure

* Pt 1 Sound calibration & headphone test (choose the quietest sound among 3)

* Pt 2 Training (press the buttons to listen to the chords, practice w/ feedback) + 
testing phase (listen to 12 chords and choose b/w major and minor for each, need to correctly answer 8 to pass)

* only analyze the response of those who pass the assessment w/in 2 tries

* Pt 3 Categorization task (jspsych) - listen to 4 blocks of 70 chords and choose b/w major and minor for each chord; 
explicit rating of instrument valence at the end

* Pt 4 Questionnaires (demographics & music experience; Qualtrics)

## Clean Data

```{r load, include = FALSE}
df_j <- read_csv("inst-cat-uc-1.csv") %>% # load exp1 data
  filter(designation != "NA" & designation != "practice-intro" & designation != "practice-resp" & 
         qualtrics_id != "NA" & qualtrics_id != "9643222579") %>% # delete pilot data
  select(participant, qualtrics_id, chord, designation, response, # delete meta data
         correct, passed_practice, block_passed_practice, practice_score, 
         instrument, valence, tuning_step, selected_major, explicit_rtg) %>% 
  # for some reason 2 participant's qualtrics id was "participant", so I recoded according to their id on Qualtrics
  mutate(qualtrics_id = if_else(participant == "ch4dg75c7th9g1", "1706631024", qualtrics_id), 
         qualtrics_id = if_else(participant == "ept8xz3drgq38w", "4475978126", qualtrics_id),
         instrument = factor(instrument, levels = c("xylophone", "trumpet", "piano", "violin", "oboe")))

df_q <- read_csv("Qualtrics_5:4.csv") %>%
  filter(row_number() > 7) %>% # delete pilot data
  select(-(2:16), -50) # delete unnecessary meta data

# join qualtrics & jspsych data by embedded qualtrics id
# discard those who do not have a matching qualtrics_id in qualtrics data, keep everyone in jspsych data
combined <- left_join(df_j, df_q, by = c("qualtrics_id" = "participant"))

# check if there are participants who wrote the wrong jspsych code
# miraculously there aren't any
combined %>%
  filter(participant != jspsych_id)
```

```{r demo-headphone}
# df for demographics
# need to find a way to summarize musical expertise
demo <- combined %>%
  filter(designation == "PRACTICE-PASSED-SUMMARY") %>%
  select(-1, -(3:6), -(10:16), -49) %>% # delete unnecessary data
  mutate(Age = as.numeric(Age))

# df for headphone test
test <- df_j %>%
  filter(designation == "headphone-test") %>%
  select(2, 6) %>%
  group_by(qualtrics_id) %>%
  summarize(n = sum(correct)) %>%
  # set 4 out of 6 as passed headphone test
  mutate(headphone = if_else(n >= 4, 1, 0),
         test_corr = n) %>%
  select(-n)

# combine headphone test pass/fail result w/ original data
demo_test <- left_join(demo, test, by = "qualtrics_id")

# see how many failed the headphone test
test %>%
  filter(headphone == 0)
```

Turns out 8 participants failed the headphone test.

```{r cat-rtg}
# df for categorization
cat <- df_j %>%
  filter(designation == "MAIN-JUDGMENT") %>%
  select(2:3, 10:13) %>%
  group_by(qualtrics_id, instrument, tuning_step) %>%
  count(selected_major)

# count function dropped rows where n == 0, but we still need those rows to calculate proportion of major cat
# so we select rows where n == 8 (all judged as major/ all judged as minor), 
# duplicate them, change their values, and bind them back to the cat df

n_8 <- cat %>%
  filter(n == 8)

n_0 <- n_8 %>%
  mutate(selected_major = if_else(selected_major == 0, 1, 0),
         n = 0)

cat_binded <- rbind(n_8, n_0, cat) %>%
  arrange(qualtrics_id, instrument, tuning_step, selected_major) %>%
  unique() %>% # delete duplicate rows (don't know why there are duplicates)
  filter(selected_major == 1) %>%
  # pct_major = each individual's proportion of choosing major for each instrument & tuning step
  #8 = # of categorizations made by each individual on each instrument w/ each tuning step
  mutate(pct_maj = n/8) %>%
  select(-n, -selected_major)
  
cat_pivoted <- cat_binded %>%
  pivot_wider(names_from = c(instrument, tuning_step), values_from = pct_maj)

# df for explicit rating
rtg <- df_j %>%
  filter(designation == "INST-VALENCE-RTG") %>%
  group_by(qualtrics_id, instrument) %>%
  count(explicit_rtg) %>%
  select(-n) 

rtg_pivoted <- rtg %>%
  pivot_wider(names_from = instrument, values_from = explicit_rtg)

# df for both combined
cat_rtg <- left_join(cat_binded, rtg)

cat_rtg_summary <- cat_rtg %>%
  group_by(instrument) %>%
  summarize(mean_pct = sum(pct_maj) / 245, # 245 = # of times each instrument appears
         mean_rtg = sum(explicit_rtg) / 245)

# both pivoted combined
cat_rtg_pivoted <- cat_pivoted %>% 
  left_join(rtg_pivoted, by = c("qualtrics_id"))

# delete the following since no need
# mutate(inst_id = case_when(instrument == "oboe" ~ 1, instrument == "violin" ~ 2,
# instrument == "piano" ~ 3, instrument == "trumpet" ~ 4, instrument == "xylophone" ~ 5))

all <- left_join(cat_rtg_pivoted, demo_test)
```

A snapshot of the data (next step: find a measure to summarize musical background)

```{r present}
glimpse(all)
```

## Demographics

```{r demo}
demo %>%
  filter(Gender != "NA") %>%
  ggplot(aes(Gender, fill = Gender)) +
  geom_bar() +
  labs(title = "Participant gender distribution",
       x = "Gender",
       y = "Number of participants") +
  theme_minimal() +
  theme(legend.position = "None")

demo %>%
  ggplot(aes(Age)) +
  geom_bar(fill = "lightblue") +
  labs(title = "Participant age distribution",
       x = "Age",
       y = "Number of participants") +
  theme_minimal()
```

## Practice Score

```{r practice}
demo %>%
  ggplot(aes(practice_score)) +
  geom_bar(fill = "lightgreen") +
  labs(title = "Practice score distribution among those who passed (almost all did)",
       subtitle = "Pass: getting ≥ 8 out of 12 correct",
       x = "Number of correct responses out of 12",
       y = "Number of participants") +
  theme_minimal()
```

## Categorization

```{r cat}
cat_binded %>%
  ggplot(aes(tuning_step, pct_maj, color = instrument)) +
  geom_smooth(se = FALSE) +
  scale_y_continuous(labels = scales::percent) +
  # need to come up w/ a better palette
  # scale_color_discrete_sequential("Viridis", rev = TRUE) + 
  labs(title = "Proportion of major choices vs. tuning step & instruments",
       x = "Tuning step", y = "Percept of major choices") +
  theme_minimal()
```

## Explicit Rating

```{r rating}
rtg %>%
  ggplot(aes(instrument, explicit_rtg, fill = instrument)) +
  geom_col() +
  labs(title = "Explicit valence rating for each instrument",
       x = "Instrument", y = "Mean rating") +
  theme_minimal() +
  theme(legend.position = "None")
```

## Compare trend between tonality judgment and explicit rating

```{r compare-cat-valence}
cat_rtg_summary %>%
  ggplot() +
  geom_line(aes(instrument, mean_pct, group = 1), color = "red") +
  geom_line(aes(instrument, mean_rtg, group = 1), color = "blue") +
  scale_y_continuous(limits = c(0.2, 3)) +
  labs(title = "Compare trend b/w tonality categorization & valence rating",
       x = "Instrument", y = "Tonality categorization (% major) | Explicit valence rating") +
  theme_minimal() +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank())

cat_rtg_summary %>%
  pivot_longer(cols = c(mean_pct, mean_rtg), names_to = "type", values_to = "value") %>%
  ggplot(aes(instrument, value, group = 1)) +
  geom_line() +
  facet_wrap(~type, scales = "free_y")
```

## Explore correlation between music background & categorization / explicit rating

TBC

## Statistical Analyses

Descriptives, exploratory plot

```{r descr}
# save data to be used
#attach(all)

# descriptives
#summary(cat_rtg)
#plot(instrument, pct_maj)
```

Correlations b/w 1) instrument's presumed valence, 2) mean explicit rating, and 3) tuning step & percent of major categorization, 
and 4)instrument's presumed valence & mean explicit rating

```{r cor}
#cor(cat_rtg$inst_id, cat_rtg$pct_maj)
#cor(cat_rtg$mean_rtg, cat_rtg$pct_maj)
#cor(cat_rtg$tuning_step, cat$pct_maj)
#cor(cat_rtg$inst_id, cat_rtg$mean_rtg)
```

Logistic regression

1) Percent major ~ instrument & tuning step

```{r log-reg}
#glm.fit <- glm(pct_maj ~ instrument + tuning_step, family = binomial) 
# family= binomial tells r to run logistic regression
#summary(glm.fit)
```

2) Percent major ~ mean explicit rating of each instrument & tuning step

```{r log-reg2}
#glm.fit2 <- glm(pct_maj ~ mean_rtg + tuning_step, family = binomial) 
# family= binomial tells r to run logistic regression
#summary(glm.fit2)
```

Linear regression

```{r lm-fit-both}
#lm.fit2 <- lm(pct_maj ~ tuning_step, data = cat)
#lm.fit <- lm(pct_maj ~ instrument + tuning_step, data = cat) 
#summary(lm.fit)
```

ANOVA exploring 1) whether adding instrument as a predictor significantly improves model, 
and 2) whether adding both predictors is significantly better than null model

```{r lm-anova}
#anova(lm.fit, lm.fit2) # adding instrument significantly improves model
#anova(lm.fit) # adding both predictors significantly better than null model
```
