---
title: "Eva Analyses"
author: "Eva Wu"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include = FALSE}
library(tidyverse)
library(ggpubr)
library(corrplot)
library(rstatix)
library(emmeans)

data <- read_csv("all.csv")
desc <- read_csv("descriptives.csv")
```

Very helpful [link](https://www.datanovia.com/en/lessons/mixed-anova-in-r/)!

## Summary Statistics

```{r descr}
# descriptives
summary(data)

# marginal means 
# instrument
desc %>%
  group_by(instrument) %>%
  summarize(mean_pct_inst = mean(mean_pct),
            sd_pct_inst = sd(mean_pct), # is this the right way to calculate sd? or should this be done in previous steps
            mean_rtg_inst = mean(mean_rtg),
            sd_rtg_inst = sd(mean_rtg))

# tuning step
desc %>%
  group_by(tuning_step) %>%
  summarize(mean_pct_tune = mean(mean_pct), 
            sd_pct_tune = sd(mean_pct),
            mean_rtg_tune = mean(mean_rtg),
            sd_rtg_tune = sd(mean_rtg))

# key
desc %>%
  group_by(chord) %>%
  summarize(mean_pct_key = mean(mean_pct), 
            sd_pct_key = sd(mean_pct),
            mean_rtg_key = mean(mean_rtg),
            sd_rtg_key = sd(mean_rtg))

data %>%
  group_by(instrument, tuning_step, chord) %>%
  get_summary_stats(pct_maj, explicit_rtg, type = "mean_sd")
```

## Visualization

```{r graph}
data %>% 
  ggplot(aes(tuning_step, pct_maj, color = instrument)) +
  geom_smooth(se = FALSE) +
  facet_wrap(~chord) +
  labs(title = "Proportion of major chord categorization across different instruments and tuning steps",
       subtitle = "compared between the key of B and C",
       x = "Tuning step (+0c ~ +100c)", y = "Proportion of major categorization") +
  theme_bw()

data %>% 
  ggplot(aes(tuning_step, pct_maj, color = chord)) +
  geom_smooth(se = FALSE) +
  facet_wrap(~instrument) +
  labs(title = "Proportion of major chord categorization across different keys and tuning steps",
       x = "Tuning step (+0c ~ +100c)", y = "Proportion of major categorization") +
  theme_bw()

data %>% 
  ggplot(aes(reorder(instrument, explicit_rtg), explicit_rtg, fill = instrument)) +
  geom_col() +
  facet_wrap(~chord) +
  labs(title = "Mean explicit valence rating across different instruments",
       subtitle = "compared between the key of B and C",
       x = "Instrument", y = "Mean explicit valence rating") +
  theme_bw()
```

## Check assumptions

### Outliers

```{r outliers}
# for cat
data_summary <- data %>%
  group_by(qualtrics_id, tuning_step) %>%
  summarize(mean_pct = mean(pct_maj))

data %>%
  group_by(qualtrics_id) %>%
  summarize(mean_pct = mean(pct_maj)) %>%
  identify_outliers(mean_pct)

# for rtg
data %>%
  group_by(qualtrics_id) %>%
  summarize(mean_rtg = mean(explicit_rtg)) %>%
  ggplot(aes(mean_rtg)) +
  geom_histogram(color = "white")

data %>%
  group_by(qualtrics_id) %>%
  summarize(mean_rtg = mean(explicit_rtg)) %>%
  identify_outliers(mean_rtg)

# examine outliers
data %>%
  filter(qualtrics_id == 1588756489 | qualtrics_id == 6323213291 | qualtrics_id == 6444402078) %>%
  select(qualtrics_id, instrument, explicit_rtg) %>%
  unique()

# ignore b/c not extreme
```

### Normality

```{r}
# violated but fine
data %>%
  group_by(instrument, tuning_step, chord) %>%
  shapiro_test(pct_maj)

ggqqplot(data, "pct_maj", ggtheme = theme_bw()) +
  facet_grid(tuning_step ~ instrument, labeller = "label_both")
```

### Homogeneity of variance

```{r}
data %>% levene_test(pct_maj ~ instrument*factor(tuning_step)*chord)
```

No need to transform for assumption violations b/c ANOVA is robust for these issues. Just report a Greenhouse-Geisser correction (epsilon * df).

The assumption of sphericity will be automatically checked during the computation of the ANOVA test using the R function anova_test() [rstatix package]. The Mauchlyâ€™s test is internally used to assess the sphericity assumption.

By using the function get_anova_table() [rstatix] to extract the ANOVA table, the Greenhouse-Geisser correction is automatically applied to factors violating the sphericity assumption.

## ANOVA

```{r}
chord.aov <- anova_test(data = data, dv = pct_maj, wid = qualtrics_id,
  within = c(instrument, tuning_step), between = chord)
get_anova_table(chord.aov) # sphericity violated but corrected w/ GG

# no main effect of chord / 3-way int (no change in main effects/int of interest), so left it out for the sake of parsimony

mus.aov <- anova_test(data = data, dv = pct_maj, wid = qualtrics_id,
  within = c(instrument, tuning_step), covariate = Inst_yr)
get_anova_table(mus.aov)

# adding musical training doesn't change effects of main interest, same as chord
# int may be due to steeper slope for more trained participants, but has nothing to do with our hypotheses

# A significant two-way interaction can be followed up by a simple main effect analysis, 
# which can be followed up by simple pairwise comparisons if significant.

aov <- anova_test(data = data, dv = pct_maj, wid = qualtrics_id,
  within = c(instrument, tuning_step))
get_anova_table(aov)
```

## Post-hoc tests

```{r post-hoc}
# post hoc for main eff of instrument
data %>%
  pairwise_t_test(
    pct_maj ~ instrument, paired = TRUE, 
    p.adjust.method = "bonferroni"
    ) %>% 
  select(-`.y.`, -p)

# post hoc for main eff of tuning
data %>%
  pairwise_t_test(
    pct_maj ~ tuning_step, paired = TRUE, 
    p.adjust.method = "bonferroni"
    ) %>% 
  select(-`.y.`, -p)

# post hoc for int
data %>%
  group_by(tuning_step) %>%
  pairwise_t_test(
    pct_maj ~ instrument, paired = TRUE, 
    p.adjust.method = "BH" # try different options
    ) %>% 
  select(-`.y.`, -p)

# table for significant rows to present
```

## Correlations b/w DV & other predictors

```{r corr}
cor.test(data$pct_maj, data$Inst)
cor.test(data$pct_maj, data$Inst_yr)
cor.test(data$pct_maj, data$music_exp)
cor.test(data$pct_maj, data$Read) #*
cor.test(data$pct_maj, data$headphone)
cor.test(data$pct_maj, data$Age)
cor.test(data$pct_maj, data$explicit_rtg) #***
```

Ability to read music and explicit valence rating were significantly correlated with percent of major categorization

```{r corrplot}
corrplot(cor(data %>%
  select(pct_maj, explicit_rtg, practice_score, Age, 16:21), use = "complete.obs"), method = "color")
# why NA?
```

## Logistic regression (wrong, should use selected_major as DV)

1) Percent major ~ instrument & tuning step

```{r log-reg}
#glm.fit <- glm(pct_maj ~ instrument + tuning_step, family = binomial) 
# family = binomial tells r to run logistic regression
#summary(glm.fit)
```

2) Percent major ~ mean explicit rating of each instrument & tuning step

```{r log-reg2}
#glm.fit2 <- glm(pct_maj ~ mean_rtg + tuning_step, family = binomial) 
# family= binomial tells r to run logistic regression
#summary(glm.fit2)
```

Linear regression

```{r lm-fit-both}
#lm.fit2 <- lm(pct_maj ~ tuning_step, data = cat)
#lm.fit <- lm(pct_maj ~ instrument + tuning_step, data = cat) 
#summary(lm.fit)
```

ANOVA exploring 1) whether adding instrument as a predictor significantly improves model, 
and 2) whether adding both predictors is significantly better than null model

```{r lm-anova}
#anova(lm.fit, lm.fit2) # adding instrument significantly improves model
#anova(lm.fit) # adding both predictors significantly better than null model
```
