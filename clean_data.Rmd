---
title: "Exploratory Analysis"
author: "Eva Wu"
output: github_document
date: '2022-05-18'
---

```{r setup, include = FALSE}
library(tidyverse)
library(colorspace)
library(ISLR2)
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

## Recap

### Hypothesis 

“Happy” instruments would make people more prone to identify the chord as major, 
while “sad” instruments might make people more prone to identify the chord as minor.

### Exploratory research questions 

1) Association between timbre and tonality judgment 

2) Association between timbre and explicit ratings of instrument valence

3) Association between tonality judgment and explicit ratings of instrument valence

4) Association between musical background and tonality judgment and/or explicit ratings of instrument valence

### Design

* IV1 (w/in-subject): instrument (happy [marimba, xylophone, vibraphone] vs. neutral [piano] vs. sad [oboe, flute, violin]) 

* IV2 (w/in-subject): tuning of middle note (5 levels, ranging from absolute minor to absolute major)

* IV3 (b/w-subject): key (Bb vs. C) (to find out absolute-pitch-related effects)

* DV: the likelihood that one categorizes a chord as major/minor 

### Procedure

* Pt 1 Sound calibration (choose the quietest sound among 3)

* Pt 2 Training (press the buttons to listen to the chords, practice w/ feedback) + testing phase (listen to 12 chords and choose b/w major and minor for each, need to correctly answer 8 to pass)

* Everyone moves on to the categorization task, but we will only analyze the response of those who pass the assessment. 

* Pt 3 Categorization task (jspsych) - listen to 4 blocks of 70 chords and choose b/w major and minor for each chord; explicit rating of instrument valence at the end

* Pt 4 Questionnaires (demographics & music experience; Qualtrics)

## Clean Data

```{r clean, include = FALSE}
df_j <- read_csv("inst-cat-uc-1.csv") %>% # load exp1 data
  filter(designation != "NA" & designation != "practice-intro" & designation != "practice-resp" & 
         qualtrics_id != "NA" & qualtrics_id != "9643222579") %>% # delete pilot data
  select(participant, qualtrics_id, chord, designation, response, # delete meta data
         correct, passed_practice, block_passed_practice, practice_score, 
         instrument, valence, tuning_step, selected_major, explicit_rtg) %>% 
  # for some reason 2 participant's qualtrics id was "participant", so I recoded according to their id on Qualtrics
  mutate(qualtrics_id = if_else(participant == "ch4dg75c7th9g1", "1706631024", qualtrics_id), 
         qualtrics_id = if_else(participant == "ept8xz3drgq38w", "4475978126", qualtrics_id),
         instrument = factor(instrument, levels = c("xylophone", "trumpet", "piano", "violin", "oboe")))

df_q <- read_csv("Qualtrics_5:4.csv") %>%
  filter(row_number() > 7) %>% # delete pilot data
  select(-(2:16), -50) # delete unnecessary meta data

# join qualtrics & jspsych data by embedded qualtrics id, discarding those who do not have a matching qualtrics_id in qualtrics data, keep everyone in jspsych data
combined <- left_join(df_j, df_q, by = c("qualtrics_id" = "participant"))

# check if there are participants who wrote the wrong jspsych code
# miraculously there aren't any
combined %>%
  filter(participant != jspsych_id)

# df for demographics
demo <- combined %>%
  filter(designation == "PRACTICE-PASSED-SUMMARY") %>%
  select(-(1:6), -(10:16)) %>%
  mutate(Age = as.numeric(Age))

# df for categorization
cat <- df_j %>%
  filter(designation == "MAIN-JUDGMENT") %>%
  select(10:13, 5) %>%
  group_by(instrument, tuning_step) %>%
  count(selected_major) %>%
  filter(selected_major == 1) %>%
  select(-3) %>%
  mutate(pct_maj = n/392, # 392 = # of categorizations made
         inst_id = case_when(instrument == "oboe" ~ 1,
                             instrument == "violin" ~ 2,
                             instrument == "piano" ~ 3,
                             instrument == "trumpet" ~ 4,
                             instrument == "xylophone" ~ 5)) 

# df for explicit rating
rtg <- df_j %>%
  filter(designation == "INST-VALENCE-RTG") %>%
  group_by(instrument) %>%
  count(explicit_rtg) %>%
  summarise(mean_rtg = sum((explicit_rtg*n)/sum(n))) 

# df for both combined
cat_rtg <- cat %>% 
  left_join(rtg, by = "instrument")
```

A snapshot of the data (still need to do further cleaning)

```{r present}
glimpse(combined)
```

## Demographics

```{r demo}
demo %>%
  filter(Gender != "NA") %>%
  ggplot(aes(Gender, fill = Gender)) +
  geom_bar() +
  labs(title = "Participant gender distribution",
       x = "Gender",
       y = "Number of participants") +
  theme_minimal() +
  theme(legend.position = "None")

demo %>%
  ggplot(aes(Age)) +
  geom_bar(fill = "lightblue") +
  labs(title = "Participant age distribution",
       x = "Age",
       y = "Number of participants") +
  theme_minimal()
```

## Practice Score

```{r practice}
demo %>%
  ggplot(aes(practice_score)) +
  geom_bar(fill = "lightgreen") +
  labs(title = "Practice score distribution among those who passed (almost all did)",
       subtitle = "Pass: getting ≥ 8 out of 12 correct",
       x = "Number of correct responses out of 12",
       y = "Number of participants") +
  theme_minimal()
```

## Categorization

```{r cat}
cat %>%
  ggplot(aes(tuning_step, pct_maj, color = instrument)) +
  geom_smooth() +
  scale_y_continuous(labels = scales::percent) +
  # need to come up w/ a better palette
  # scale_color_discrete_sequential("Viridis", rev = TRUE) + 
  labs(title = "Proportion of major choices vs. tuning step & instruments",
       x = "Tuning step", y = "Percept of major choices") +
  theme_minimal()
```

## Explicit Rating

```{r rating}
rtg %>%
  ggplot(aes(instrument, mean_rtg, fill = instrument)) +
  geom_col() +
  labs(title = "Explicit valence rating for each instrument",
       x = "Instrument", y = "Mean rating") +
  theme_minimal() +
  theme(legend.position = "None")
```

## Compare trend between tonality judgment and explicit rating

```{r compare-valence}
cat %>%
  group_by(instrument) %>%
  summarise(mean_pct = sum(pct_maj) / 5) %>%
  ggplot() +
  geom_line(mapping = aes(instrument, mean_pct, group = 1), color = "red") +
  geom_line(rtg, mapping = aes(instrument, mean_rtg, group = 1), color = "blue") +
  scale_y_continuous(limits = c(0.2, 3)) +
  labs(title = "Compare trend b/w tonality categorization & valence rating",
       x = "Instrument", y = "Tonality categorization (% major) | Explicit valence rating") +
  theme_minimal() +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank())
```

## Explore correlation between music background & categorization / explicit rating

TBC

## Statistical Analyses

Descriptives, exploratory plot

```{r descr}
# save data to be used
attach(cat_rtg)

# descriptives
summary(cat_rtg)
plot(instrument, pct_maj)
```

Correlations b/w 1) instrument's presumed valence, 2) mean explicit rating, and 3) tuning step & percent of major categorization, 
and 4)instrument's presumed valence & mean explicit rating

```{r cor}
cor(cat_rtg$inst_id, cat_rtg$pct_maj)
cor(cat_rtg$mean_rtg, cat_rtg$pct_maj)
cor(cat_rtg$tuning_step, cat$pct_maj)
cor(cat_rtg$inst_id, cat_rtg$mean_rtg)
```

Logistic regression

1) Percent major ~ instrument & tuning step

```{r log-reg}
glm.fit <- glm(pct_maj ~ instrument + tuning_step, family = binomial) 
# family= binomial tells r to run logistic regression
summary(glm.fit)
```

2) Percent major ~ mean explicit rating of each instrument & tuning step

```{r log-reg2}
glm.fit2 <- glm(pct_maj ~ mean_rtg + tuning_step, family = binomial) 
# family= binomial tells r to run logistic regression
summary(glm.fit2)
```

Linear regression

```{r lm-fit-both}
lm.fit2 <- lm(pct_maj ~ tuning_step, data = cat)
lm.fit <- lm(pct_maj ~ instrument + tuning_step, data = cat) 
summary(lm.fit)
```

ANOVA exploring 1) whether adding instrument as a predictor significantly improves model, 
and 2) whether adding both predictors is significantly better than null model

```{r lm-anova}
anova(lm.fit, lm.fit2) # adding instrument significantly improves model
anova(lm.fit) # adding both predictors significantly better than null model
```
